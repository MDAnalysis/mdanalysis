

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tests in MDAnalysis &mdash; MDAnalysis User Guide  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/msmb.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/mdanalysis-logo.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
        <script src="_static/js/versions.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within MDAnalysis User Guide  documentation"
          href="_static/opensearch.xml"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="References" href="references.html" />
    <link rel="prev" title="Module imports in MDAnalysis" href="module_imports.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/user_guide.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/quickstart.html">Quick start guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently asked questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/README.html">Examples</a></li>
</ul>
<p class="caption"><span class="caption-text">Data structures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="universe.html">Universe</a></li>
<li class="toctree-l1"><a class="reference internal" href="atomgroup.html">AtomGroup</a></li>
<li class="toctree-l1"><a class="reference internal" href="groups_of_atoms.html">Groups of atoms</a></li>
<li class="toctree-l1"><a class="reference internal" href="selections.html">Atom selection language</a></li>
<li class="toctree-l1"><a class="reference internal" href="topology_system.html">The topology system</a></li>
</ul>
<p class="caption"><span class="caption-text">Trajectories</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="trajectories/trajectories.html">Trajectories</a></li>
<li class="toctree-l1"><a class="reference internal" href="trajectories/slicing_trajectories.html">Slicing trajectories</a></li>
<li class="toctree-l1"><a class="reference internal" href="trajectories/transformations.html">On-the-fly transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="units.html">Units and constants</a></li>
</ul>
<p class="caption"><span class="caption-text">Input/output</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reading_and_writing.html">Reading and writing files</a></li>
<li class="toctree-l1"><a class="reference internal" href="formats/index.html">Format overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="formats/guessing.html">Guessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="formats/auxiliary.html">Auxiliary files</a></li>
<li class="toctree-l1"><a class="reference internal" href="formats/selection_exporters.html">Selection exporters</a></li>
<li class="toctree-l1"><a class="reference internal" href="formats/format_reference.html">Format reference</a></li>
</ul>
<p class="caption"><span class="caption-text">Analysis</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples/analysis/README.html">Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/analysis/README.html#alignments-and-rms-fitting">Alignments and RMS fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/analysis/README.html#distances-and-contacts">Distances and contacts</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/analysis/README.html#trajectory-similarity">Trajectory similarity</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/analysis/README.html#structure">Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/analysis/README.html#volumetric-analyses">Volumetric analyses</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/analysis/README.html#dimension-reduction">Dimension reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/analysis/README.html#polymers-and-membranes">Polymers and membranes</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/analysis/custom_trajectory_analysis.html">Writing your own trajectory analysis</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="standard_selections.html">Standard residues in MDAnalysis selections</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced_topology.html">Advanced topology concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Example data</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributing</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to MDAnalysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing_code.html">Contributing to the main codebase</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing_docs.html">Contributing to the user guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="preparing_releases_and_hotfixes.html">Preparing a release</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_imports.html">Module imports in MDAnalysis</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tests in MDAnalysis</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#running-the-test-suite">Running the test suite</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#testing-in-parallel">Testing in parallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="#test-coverage">Test coverage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#continuous-integration-tools">Continuous Integration tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#appveyor">Appveyor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#travis">Travis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#codecov">Codecov</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#writing-new-tests">Writing new tests</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#general-conventions">General conventions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#assertions">Assertions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#testing-exceptions-and-warnings">Testing exceptions and warnings</a></li>
<li class="toctree-l4"><a class="reference internal" href="#failing-tests">Failing tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="#skipping-tests">Skipping tests</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fixtures">Fixtures</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testing-the-same-function-with-different-inputs">Testing the same function with different inputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#temporary-files-and-directories">Temporary files and directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-imports">Module imports</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tests-for-analysis-and-visualization-modules">Tests for analysis and visualization modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-test-data-files">Using test data files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MDAnalysis User Guide</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Tests in MDAnalysis</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/testing.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tests-in-mdanalysis">
<span id="testing"></span><h1>Tests in MDAnalysis<a class="headerlink" href="#tests-in-mdanalysis" title="Permalink to this headline">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parts of this page came from the <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/contributing.html">Contributing to pandas</a> guide.</p>
</div>
<p>Whenever you add new code, you should create an appropriate test case that checks that your code is working as it should. This is very important because:</p>
<blockquote>
<div><ol class="arabic">
<li><p>Firstly, it ensures that your code works as expected, i.e.</p>
<blockquote>
<div><ul class="simple">
<li><p>it succeeds in your test cases <em>and</em></p></li>
<li><p>it fails predictably</p></li>
</ul>
</div></blockquote>
</li>
<li><p>More importantly, in the future we can always test that it is still working correctly. Unit tests are a crucial component of proper software engineering (see e.g. <a class="reference external" href="http://software-carpentry.org/4_0/test">Software Carpentry on Testing</a>) and a large (and growing) test suite is one of the strengths of MDAnalysis.</p></li>
</ol>
</div></blockquote>
<p>Adding tests is one of the most common requests after code is pushed to MDAnalysis.  Therefore,
it is worth getting in the habit of writing tests ahead of time so this is never an issue. We strive for ≥90% our code to be covered by tests.</p>
<p>We strongly encourage contributors to embrace
<a class="reference external" href="http://en.wikipedia.org/wiki/Test-driven_development">test-driven development</a>.
This development process “relies on the repetition of a very short development cycle:
first the developer writes an (initially failing) automated test case that defines a desired
improvement or new function, then produces the minimum amount of code to pass that test.”
So, before actually writing any code, you should write your tests.  Often the test can be
taken from the original GitHub issue.  However, it is always worth considering additional
use cases and writing corresponding tests.</p>
<p>Like many packages, MDAnalysis uses <a class="reference external" href="http://doc.pytest.org/en/latest/">pytest</a> and some of the <a class="reference external" href="http://docs.scipy.org/doc/numpy/reference/routines.testing.html">numpy.testing</a> framework.</p>
<div class="section" id="running-the-test-suite">
<span id="run-test-suite"></span><h2>Running the test suite<a class="headerlink" href="#running-the-test-suite" title="Permalink to this headline">¶</a></h2>
<p>It is recommended that you run the tests from the <code class="docutils literal notranslate"><span class="pre">mdanalysis/testsuite/MDAnalysisTests/</span></code> directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> testsuite/MDAnalysisTests
pytest  --disable-pytest-warnings
</pre></div>
</div>
<p>All tests should pass: no <strong>FAIL</strong> or <strong>ERROR</strong> cases should be triggered; <em>SKIPPED</em> or <em>XFAIL</em> are ok. For anything that fails or gives an error, ask on the <a class="reference external" href="http://groups.google.com/group/mdnalysis-discussion">mdnalysis-discussion</a> mailing list or raise an issue on the <a class="reference external" href="https://github.com/MDAnalysis/mdanalysis/issues">Issue Tracker</a>.</p>
<p>We use the <code class="docutils literal notranslate"><span class="pre">--disable-pytest-warnings</span></code> when the whole testsuite is running, as pytest raises a lot of false positives when we warn users about missing topology attributes. When running single tests or only single modules, consider running the tests <em>with</em> warnings enabled (i.e. without <code class="docutils literal notranslate"><span class="pre">--disable-pytest-warnings</span></code>). This allows you to see if you trigger any un-caught deprecation warnings or other warnings in libraries we use.</p>
<p>To run specific tests just specify the path to the test file:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest testsuite/MDAnalysisTests/analysis/test_align.py
</pre></div>
</div>
<p>Specific test classes inside test files, and even specific test methods, can also be specified:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test the entire TestContactMatrix class</span>
pytest testsuite/MDAnalysisTests/analysis/test_analysis.py::TestContactMatrix

<span class="c1"># Test only test_sparse in the TestContactMatrix class</span>
pytest testsuite/MDAnalysisTests/analysis/test_analysis.py::TestContactMatrix::test_sparse
</pre></div>
</div>
<p>This is very useful when you add a new test and want to check if it passes. However, before you push your code to GitHub, make sure that your test case runs and that <em>all other test cases are still passing</em>.</p>
<div class="section" id="testing-in-parallel">
<h3>Testing in parallel<a class="headerlink" href="#testing-in-parallel" title="Permalink to this headline">¶</a></h3>
<p>Running the tests serially can take some time, depending on the performance of your computer. You can speed this up by using the plugin <a class="reference external" href="https://github.com/pytest-dev/pytest-xdist">pytest-xdist</a> to run tests in parallel by specifying the <code class="docutils literal notranslate"><span class="pre">--numprocesses</span></code> option:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install pytest-xdist
pytest --disable-pytest-warnings --numprocesses <span class="m">4</span>
</pre></div>
</div>
<p>You can try increasing the number of processes to speed up the test run. The number of processes you can use depends on your machine.</p>
</div>
<div class="section" id="test-coverage">
<h3>Test coverage<a class="headerlink" href="#test-coverage" title="Permalink to this headline">¶</a></h3>
<p>The tool <a class="reference external" href="https://github.com/pytest-dev/pytest-cov">pytest-cov</a>  can be used to generate the coverage report locally:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install pytest-cov
pytest --cov<span class="o">=</span>MDAnalysis
</pre></div>
</div>
<p>Note: You can use the <code class="docutils literal notranslate"><span class="pre">--numprocesses</span></code> flag to run tests in parallel with the above command too. This will print the coverage statistic for every module in MDAnalysis at the end of a run. To get detailed line by
line statistics you can add the <code class="docutils literal notranslate"><span class="pre">--cov-report=html</span></code> flag. This will create a <code class="docutils literal notranslate"><span class="pre">htmlcov</span></code> folder (in the directory you run the command from) and there will be an <code class="docutils literal notranslate"><span class="pre">index.html</span></code> file in this folder. Open this file in your browser and you will be able to see overall statistics and detailed line coverage for each file.</p>
</div>
</div>
<div class="section" id="continuous-integration-tools">
<span id="continuous-integration"></span><h2>Continuous Integration tools<a class="headerlink" href="#continuous-integration-tools" title="Permalink to this headline">¶</a></h2>
<p>When you submit your pull request, several continuous integration tools will run a suite of tests. These should all pass before your code can be merged into MDAnalysis. You can check tests locally by <a class="reference internal" href="#run-test-suite"><span class="std std-ref">running the test suite</span></a>.</p>
<p>If your pull request fails immediately with an <a class="reference internal" href="#appveyor"><span class="std std-ref">Appveyor</span></a> error, it is likely that you have merge conflicts with the latest code in the <code class="docutils literal notranslate"><span class="pre">develop</span></code> branch. <a class="reference internal" href="contributing_code.html#rebase-code"><span class="std std-ref">Rebase your code</span></a> and update your branch by pushing your changes.</p>
<p>If you get an error with <a class="reference internal" href="#travis"><span class="std std-ref">Travis</span></a>, it is likely that you’ve failed a particular test. You should update your code and push again.</p>
<p>If you get <a class="reference internal" href="#codecov"><span class="std std-ref">Codecov</span></a> errors, this means that your changes have not been adequately tested. Add new tests that address the “missed” lines, and push again.</p>
<p>Ideally, you want all tests to pass. This will look like:</p>
<blockquote>
<div><img alt="_images/ci_checks_passed.png" src="_images/ci_checks_passed.png" />
</div></blockquote>
<div class="section" id="appveyor">
<span id="id1"></span><h3>Appveyor<a class="headerlink" href="#appveyor" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="#appveyor">AppVeyor</a> is a continuous integration/continuous deployment service. MDAnalysis uses it for <a class="reference external" href="https://ci.appveyor.com/project/orbeckst/mdanalysis">testing builds on Windows</a>.</p>
<p>Builds are configured in the file <code class="docutils literal notranslate"><span class="pre">.appveyor.yml</span></code>. If you add a new dependency to MDAnalysis, you will need to add it to the <code class="docutils literal notranslate"><span class="pre">$CONDA_DEPENDENCIES</span></code> or <code class="docutils literal notranslate"><span class="pre">$PIP_DEPENDENCIES</span></code> in <code class="docutils literal notranslate"><span class="pre">.appveyor.yml</span></code> to pass tests.</p>
</div>
<div class="section" id="travis">
<span id="id2"></span><h3>Travis<a class="headerlink" href="#travis" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://travis-ci.com/MDAnalysis/mdanalysis">Travis is a continuous integration service</a> for Linux and MacOS. MDAnalysis uses it for exhaustive testing on Linux systems, and some testing on MacOS. If you add a new dependency to MDAnalysis, you will need to add it to the <code class="docutils literal notranslate"><span class="pre">$CONDA_DEPENDENCIES</span></code> or <code class="docutils literal notranslate"><span class="pre">$PIP_DEPENDENCIES</span></code> in <code class="docutils literal notranslate"><span class="pre">.travis.yml</span></code> to pass tests.</p>
</div>
<div class="section" id="codecov">
<span id="id3"></span><h3>Codecov<a class="headerlink" href="#codecov" title="Permalink to this headline">¶</a></h3>
<p>Code coverage measures how many lines, and which lines, of code are executed by a test suite. Codecov is a service that turns code coverage reports into a single visual report. Each line is described as one of three categories:</p>
<blockquote>
<div><ul class="simple">
<li><p>a <strong>hit</strong> indicates that the source code was executed by the test suite.</p></li>
<li><p>a <strong>partial</strong> indicates that the source code was not fully executed by the test suite; there are remaining branches that were not executed.</p></li>
<li><p>a <strong>miss</strong> indicates that the source code was not executed by the test suite.</p></li>
</ul>
</div></blockquote>
<p>Coverage is the ratio of <code class="docutils literal notranslate"><span class="pre">hits</span> <span class="pre">/</span> <span class="pre">(sum</span> <span class="pre">of</span> <span class="pre">hit</span> <span class="pre">+</span> <span class="pre">partial</span> <span class="pre">+</span> <span class="pre">miss)</span></code>. See the <a class="reference external" href="https://docs.codecov.io/docs/about-code-coverage">Codecov documentation</a> for more information.</p>
<p>MDAnalysis aims for 90% code coverage; your pull request will fail the Codecov check if the coverage falls below 85%. You can increase coverage by writing futher tests.</p>
<p>On your pull request, Codecov will leave a comment with three sections:</p>
<blockquote>
<div><ul>
<li><p>a visual map of the areas with coverage changes</p>
<blockquote>
<div><img alt="_images/codecov_report_map.png" src="_images/codecov_report_map.png" />
</div></blockquote>
</li>
<li><p>a summary of changes in coverage</p>
<blockquote>
<div><img alt="_images/codecov_report_summary.png" src="_images/codecov_report_summary.png" />
</div></blockquote>
</li>
<li><p>a list of files with changes</p>
<blockquote>
<div><img alt="_images/codecov_report_files.png" src="_images/codecov_report_files.png" />
</div></blockquote>
</li>
</ul>
</div></blockquote>
<p>Clicking on one of those files will show the Codecov <span class="guilabel">Diff</span> view, highlighting the lines of code that have been missed by tests. In the image below, the column on the left hand side shows hits (green) and misses (red); the lighter colours highlighting the code show lines added (light green) or removed (light red).</p>
<blockquote>
<div><img alt="_images/codecov_diff.png" src="_images/codecov_diff.png" />
</div></blockquote>
<p>Changing to the <span class="guilabel">Coverage Changes</span> view highlights how your additions have changed the test coverage. See the <a class="reference external" href="https://docs.codecov.io/docs/viewing-source-code">documentation for viewing source code</a> for more information.</p>
<blockquote>
<div><img alt="_images/codecov_coverage_changes.png" src="_images/codecov_coverage_changes.png" />
</div></blockquote>
</div>
</div>
<div class="section" id="writing-new-tests">
<span id="write-new-tests"></span><h2>Writing new tests<a class="headerlink" href="#writing-new-tests" title="Permalink to this headline">¶</a></h2>
<p>Tests are organised by top-level module. Each file containing tests must start with <code class="docutils literal notranslate"><span class="pre">test_</span></code>. The tests themselves also have to follow the appropriate naming and organisational conventions.</p>
<p>Use classes to group tests if it makes sense (e.g., if the test class will be inherited by another test class and the code can be reused). We prefer subclassing over parametrizing classes (for examples, have a look at the <code class="docutils literal notranslate"><span class="pre">MDAnalysisTests/topology</span></code> module, where each class often tests a different file). For tests that are standalone, leave them as plain functions.</p>
<div class="section" id="general-conventions">
<span id="testing-conventions"></span><h3>General conventions<a class="headerlink" href="#general-conventions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="assertions">
<h4>Assertions<a class="headerlink" href="#assertions" title="Permalink to this headline">¶</a></h4>
<p>Use plain <code class="docutils literal notranslate"><span class="pre">assert</span></code> statements for comparing single values, e.g.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_foo_is_length_3</span><span class="p">(</span><span class="n">foo</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">foo</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
</pre></div>
</div>
<p>To check equality up to a certain precision (e.g. floating point numbers and iterables of floats), use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_almost_equal.html#numpy.testing.assert_almost_equal" title="(in NumPy v1.18)"><code class="xref py py-func docutils literal notranslate"><span class="pre">assert_almost_equal()</span></code></a> from <a class="reference external" href="https://numpy.org/doc/stable/reference/routines.testing.html#module-numpy.testing" title="(in NumPy v1.18)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy.testing</span></code></a>. Do not manually round off the value and use plain <code class="docutils literal notranslate"><span class="pre">assert</span></code> statements. Do not use <code class="docutils literal notranslate"><span class="pre">pytest.approx</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.testing</span> <span class="kn">import</span> <span class="n">assert_almost_equal</span>

<span class="k">def</span> <span class="nf">test_equal_coordinates</span><span class="p">():</span>
    <span class="n">ref</span> <span class="o">=</span> <span class="n">mda</span><span class="o">.</span><span class="n">Universe</span><span class="p">(</span><span class="n">PSF</span><span class="p">,</span> <span class="n">PDB_small</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">mda</span><span class="o">.</span><span class="n">Universe</span><span class="p">(</span><span class="n">PDB_small</span><span class="p">)</span>
    <span class="n">assert_almost_equal</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">atoms</span><span class="o">.</span><span class="n">positions</span><span class="p">,</span> <span class="n">ref</span><span class="o">.</span><span class="n">atoms</span><span class="o">.</span><span class="n">positions</span><span class="p">)</span>
</pre></div>
</div>
<p>To test for exact equality (e.g. integers, booleans, strings), use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_equal.html#numpy.testing.assert_equal" title="(in NumPy v1.18)"><code class="xref py py-func docutils literal notranslate"><span class="pre">assert_equal()</span></code></a> from <a class="reference external" href="https://numpy.org/doc/stable/reference/routines.testing.html#module-numpy.testing" title="(in NumPy v1.18)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy.testing</span></code></a>. As with <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_almost_equal.html#numpy.testing.assert_almost_equal" title="(in NumPy v1.18)"><code class="xref py py-func docutils literal notranslate"><span class="pre">assert_almost_equal()</span></code></a>, this should be used for iterables of exact values as well. Do not iterate over and compare every single value.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.testing</span> <span class="kn">import</span> <span class="n">assert_equal</span>

<span class="k">def</span> <span class="nf">test_equal_arrays</span><span class="p">(</span><span class="n">array1</span><span class="p">,</span> <span class="n">array2</span><span class="p">):</span>
    <span class="n">assert_equal</span><span class="p">(</span><span class="n">array1</span><span class="p">,</span> <span class="n">array2</span><span class="p">)</span>
</pre></div>
</div>
<p>Do not use <code class="docutils literal notranslate"><span class="pre">assert_array_equal</span></code> or <code class="docutils literal notranslate"><span class="pre">assert_array_almost_equal</span></code> from <a class="reference external" href="https://numpy.org/doc/stable/reference/routines.testing.html#module-numpy.testing" title="(in NumPy v1.18)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy.testing</span></code></a> to compare array/array-like data structures. Instead, use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_equal.html#numpy.testing.assert_equal" title="(in NumPy v1.18)"><code class="xref py py-func docutils literal notranslate"><span class="pre">assert_equal()</span></code></a> or <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_almost_equal.html#numpy.testing.assert_almost_equal" title="(in NumPy v1.18)"><code class="xref py py-func docutils literal notranslate"><span class="pre">assert_almost_equal()</span></code></a>. The former set of functions equate arrays and scalars, while the latter do not:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">from</span> <span class="nn">numpy.testing</span> <span class="kn">import</span> <span class="n">assert_equal</span><span class="p">,</span> <span class="n">assert_array_equal</span>

<span class="gp">In [2]: </span><span class="n">assert_array_equal</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

<span class="gp">In [3]: </span><span class="n">assert_equal</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
<span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AssertionError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">3</span><span class="o">-</span><span class="n">fa6316479812</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">assert_equal</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

<span class="nn">~/miniconda/envs/mda-user-guide/lib/python3.8/site-packages/numpy/testing/_private/utils.py</span> in <span class="ni">assert_equal</span><span class="nt">(actual, desired, err_msg, verbose)</span>
<span class="g g-Whitespace">    </span><span class="mi">379</span>     <span class="c1"># isscalar test to check cases such as [np.nan] != np.nan</span>
<span class="g g-Whitespace">    </span><span class="mi">380</span>     <span class="k">if</span> <span class="n">isscalar</span><span class="p">(</span><span class="n">desired</span><span class="p">)</span> <span class="o">!=</span> <span class="n">isscalar</span><span class="p">(</span><span class="n">actual</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">381</span>         <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">382</span> 
<span class="g g-Whitespace">    </span><span class="mi">383</span>     <span class="k">try</span><span class="p">:</span>

<span class="ne">AssertionError</span>: 
<span class="n">Items</span> <span class="n">are</span> <span class="ow">not</span> <span class="n">equal</span><span class="p">:</span>
 <span class="n">ACTUAL</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
 <span class="n">DESIRED</span><span class="p">:</span> <span class="mi">1</span>
</pre></div>
</div>
<p>Do not use anything from <a class="reference external" href="https://numpy.org/doc/stable/reference/routines.testing.html#module-numpy.testing" title="(in NumPy v1.18)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy.testing</span></code></a> that depends on <code class="docutils literal notranslate"><span class="pre">nose</span></code>, such as <code class="docutils literal notranslate"><span class="pre">assert_raises</span></code>.</p>
</div>
<div class="section" id="testing-exceptions-and-warnings">
<h4>Testing exceptions and warnings<a class="headerlink" href="#testing-exceptions-and-warnings" title="Permalink to this headline">¶</a></h4>
<p>Do not use <code class="docutils literal notranslate"><span class="pre">assert_raises</span></code> from <a class="reference external" href="https://numpy.org/doc/stable/reference/routines.testing.html#module-numpy.testing" title="(in NumPy v1.18)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy.testing</span></code></a> or the <code class="docutils literal notranslate"><span class="pre">pytest.mark.raises</span></code> decorator to test for particular exceptions. Instead, use context managers:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_for_error</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">IndexError</span><span class="p">):</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">test_for_warning</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">warns</span><span class="p">(</span><span class="ne">DeprecationWarning</span><span class="p">):</span>
        <span class="n">deprecated_function</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="failing-tests">
<h4>Failing tests<a class="headerlink" href="#failing-tests" title="Permalink to this headline">¶</a></h4>
<p>To mark an expected failure, use <a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#pytest.mark.xfail" title="(in pytest v5.4.1.dev327+ge377c884)"><code class="xref py py-func docutils literal notranslate"><span class="pre">pytest.mark.xfail()</span></code></a> decorator:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">xfail</span>
<span class="k">def</span> <span class="nf">tested_expected_failure</span><span class="p">():</span>
    <span class="k">assert</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
</div>
<p>To manually fail a test, make a call to <a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#pytest.fail" title="(in pytest v5.4.1.dev327+ge377c884)"><code class="xref py py-func docutils literal notranslate"><span class="pre">pytest.fail()</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_open</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tmpdir</span><span class="p">):</span>
    <span class="n">outfile</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">tmpdir</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;lammps-writer-test.dcd&#39;</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">mda</span><span class="o">.</span><span class="n">coordinates</span><span class="o">.</span><span class="n">LAMMPS</span><span class="o">.</span><span class="n">DCDWriter</span><span class="p">(</span><span class="n">outfile</span><span class="p">,</span> <span class="n">n_atoms</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
            <span class="k">pass</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">fail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="skipping-tests">
<h4>Skipping tests<a class="headerlink" href="#skipping-tests" title="Permalink to this headline">¶</a></h4>
<p>To skip tests based on a condition, use <a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#pytest.mark.skipif" title="(in pytest v5.4.1.dev327+ge377c884)"><code class="xref py py-func docutils literal notranslate"><span class="pre">pytest.mark.skipif(condition)</span></code></a> decorator:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">shares_memory</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">shares_memory</span> <span class="o">=</span> <span class="kc">False</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">skipif</span><span class="p">(</span><span class="n">shares_memory</span> <span class="o">==</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">reason</span><span class="o">=</span><span class="s1">&#39;old numpy lacked shares_memory&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_positions_share_memory</span><span class="p">(</span><span class="n">original_and_copy</span><span class="p">):</span>
    <span class="c1"># check that the memory in Timestep objects is unique</span>
    <span class="n">original</span><span class="p">,</span> <span class="n">copy</span> <span class="o">=</span> <span class="n">original_and_copy</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">shares_memory</span><span class="p">(</span><span class="n">original</span><span class="o">.</span><span class="n">ts</span><span class="o">.</span><span class="n">positions</span><span class="p">,</span> <span class="n">copy</span><span class="o">.</span><span class="n">ts</span><span class="o">.</span><span class="n">positions</span><span class="p">)</span>
</pre></div>
</div>
<p>To skip a test if a module is not available for importing, use <a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#pytest.importorskip" title="(in pytest v5.4.1.dev327+ge377c884)"><code class="xref py py-func docutils literal notranslate"><span class="pre">pytest.importorskip('module_name')</span></code></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_write_trajectory_netCDF4</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">universe</span><span class="p">,</span> <span class="n">outfile</span><span class="p">):</span>
    <span class="n">pytest</span><span class="o">.</span><span class="n">importorskip</span><span class="p">(</span><span class="s2">&quot;netCDF4&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_write_trajectory</span><span class="p">(</span><span class="n">universe</span><span class="p">,</span> <span class="n">outfile</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="fixtures">
<h3>Fixtures<a class="headerlink" href="#fixtures" title="Permalink to this headline">¶</a></h3>
<p>Use <a class="reference external" href="https://docs.pytest.org/en/latest/fixture.html">fixtures</a> as much as possible to reuse “resources” between test methods/functions. Pytest fixtures are functions that run before each test function that uses that fixture. A fixture is typically set up with the <a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#pytest.fixture" title="(in pytest v5.4.1.dev327+ge377c884)"><code class="xref py py-func docutils literal notranslate"><span class="pre">pytest.fixture()</span></code></a> decorator, over a function that returns the object you need:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">universe</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mda</span><span class="o">.</span><span class="n">Universe</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ref_filename</span><span class="p">)</span>
</pre></div>
</div>
<p>A function can use a fixture by including its name in its arguments:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_pdb_write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">universe</span><span class="p">):</span>
    <span class="n">universe</span><span class="o">.</span><span class="n">atoms</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;outfile.pdb&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The rule of thumb is to use the largest possible scope for the fixture to save time. A fixture declared with a class scope will run once per class; a fixture declared with a module scope will only run once per module. The default scope is <code class="docutils literal notranslate"><span class="pre">&quot;function&quot;</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s1">&#39;class&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">universe</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mda</span><span class="o">.</span><span class="n">Universe</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ref_filename</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="testing-the-same-function-with-different-inputs">
<h3>Testing the same function with different inputs<a class="headerlink" href="#testing-the-same-function-with-different-inputs" title="Permalink to this headline">¶</a></h3>
<p>Use the <code class="xref py py-func docutils literal notranslate"><span class="pre">pytest.mark.parametrize()</span></code> decorator to test the same function for different inputs rather than looping. These can be stacked:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span><span class="s1">&#39;pbc&#39;</span><span class="p">,</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span><span class="s1">&#39;name, compound&#39;</span><span class="p">,</span> <span class="p">((</span><span class="s1">&#39;molnums&#39;</span><span class="p">,</span> <span class="s1">&#39;molecules&#39;</span><span class="p">),</span>
                                            <span class="p">(</span><span class="s1">&#39;fragindices&#39;</span><span class="p">,</span> <span class="s1">&#39;fragments&#39;</span><span class="p">)))</span>
<span class="c1"># fragment is a fixture defined earlier</span>
<span class="k">def</span> <span class="nf">test_center_of_mass_compounds_special</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fragment</span><span class="p">,</span>
                                          <span class="n">pbc</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">compound</span><span class="p">):</span>
    <span class="n">ref</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">center_of_mass</span><span class="p">()</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">fragment</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
    <span class="n">com</span> <span class="o">=</span> <span class="n">fragment</span><span class="o">.</span><span class="n">center_of_mass</span><span class="p">(</span><span class="n">pbc</span><span class="o">=</span><span class="n">pbc</span><span class="p">,</span> <span class="n">compound</span><span class="o">=</span><span class="n">compound</span><span class="p">)</span>
    <span class="n">assert_almost_equal</span><span class="p">(</span><span class="n">com</span><span class="p">,</span> <span class="n">ref</span><span class="p">,</span> <span class="n">decimal</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>The code above runs <code class="docutils literal notranslate"><span class="pre">test_center_of_mass_compounds_special</span></code> 4 times with the following parameters:</p>
<blockquote>
<div><ul class="simple">
<li><p>pbc = <code class="docutils literal notranslate"><span class="pre">True</span></code>, name = <code class="docutils literal notranslate"><span class="pre">'molnums'</span></code>, compound = <code class="docutils literal notranslate"><span class="pre">'molecules'</span></code></p></li>
<li><p>pbc = <code class="docutils literal notranslate"><span class="pre">True</span></code>, name = <code class="docutils literal notranslate"><span class="pre">'fragindices'</span></code>, compound = <code class="docutils literal notranslate"><span class="pre">'fragments'</span></code></p></li>
<li><p>pbc = <code class="docutils literal notranslate"><span class="pre">False</span></code>, name = <code class="docutils literal notranslate"><span class="pre">'molnums'</span></code>, compound = <code class="docutils literal notranslate"><span class="pre">'molecules'</span></code></p></li>
<li><p>pbc = <code class="docutils literal notranslate"><span class="pre">False</span></code>, name = <code class="docutils literal notranslate"><span class="pre">'fragindices'</span></code>, compound = <code class="docutils literal notranslate"><span class="pre">'fragments'</span></code></p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="temporary-files-and-directories">
<h3>Temporary files and directories<a class="headerlink" href="#temporary-files-and-directories" title="Permalink to this headline">¶</a></h3>
<p>Do not use <a class="reference external" href="https://docs.python.org/3/library/os.html#os.chdir" title="(in Python v3.8)"><code class="xref py py-func docutils literal notranslate"><span class="pre">os.chdir()</span></code></a> to change directories in tests, because it can break the tests in really weird ways (see <a class="reference external" href="https://github.com/MDAnalysis/mdanalysis/issues/556">Issue 556</a>). To use a temporary directory as the working directory, use the <code class="xref py py-func docutils literal notranslate"><span class="pre">tmpdir.as_cwd()</span></code> context manager instead:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_write_no_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">tmpdir</span><span class="p">):</span> <span class="c1"># tmpdir is an in-built fixture</span>
    <span class="k">with</span> <span class="n">tmpdir</span><span class="o">.</span><span class="n">as_cwd</span><span class="p">():</span>
        <span class="n">u</span><span class="o">.</span><span class="n">atoms</span><span class="o">.</span><span class="n">write</span><span class="p">()</span>
</pre></div>
</div>
<p>To create a temporary file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">outfile</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">):</span>
    <span class="n">temp_file</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">tmpdir</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;test.pdb&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="section" id="module-imports">
<h4>Module imports<a class="headerlink" href="#module-imports" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="module_imports.html#module-imports-in-tests"><span class="std std-ref">Do not use relative imports</span></a> in test files, as it means that tests can no longer be run from inside the test directory. Instead, use absolute imports.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">.datafiles</span> <span class="kn">import</span> <span class="n">PDB</span>  <span class="c1"># this is relative and will break!</span>
<span class="kn">from</span> <span class="nn">MDAnalysisTests.datafiles</span> <span class="kn">import</span> <span class="n">PDB</span>  <span class="c1"># use this instead</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="tests-for-analysis-and-visualization-modules">
<h2>Tests for analysis and visualization modules<a class="headerlink" href="#tests-for-analysis-and-visualization-modules" title="Permalink to this headline">¶</a></h2>
<p>Tests for analysis classes and functions should at a minimum perform regression tests, i.e., run on input and compare to values generated when the code was added so that we know when the output changes in the future. (Even better are tests that test for absolute correctness of results, but regression tests are the minimum requirement.)</p>
<p>Any code in <code class="xref py py-mod docutils literal notranslate"><span class="pre">MDAnalysis.analysis</span></code> that does not have substantial testing (at least 70% coverage) will be moved to a special <code class="xref py py-mod docutils literal notranslate"><span class="pre">MDAnalysis.analysis.legacy</span></code> module by release 1.0.0. This legacy module will come with its own warning that this is essentially unmaintained functionality, that is still provided because there is no alternative. Legacy packages that receive sufficient upgrades in testing can come back to the normal <code class="xref py py-mod docutils literal notranslate"><span class="pre">MDAnalysis.analysis</span></code> name space.</p>
<p>No consensus has emerged yet how to best test visualization code. At least minimal tests that run the code are typically requested.</p>
</div>
<div class="section" id="using-test-data-files">
<h2>Using test data files<a class="headerlink" href="#using-test-data-files" title="Permalink to this headline">¶</a></h2>
<p>If possible, re-use the existing data files in MDAnalysis for tests; this helps to keep the (separate) MDAnalysisTests package small. If new files are required (e.g. for a new coordinate Reader/Writer) then:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Use small files (e.g. trajectories with only a few frames and a small system).</p></li>
<li><p>Make sure that the data are <em>not confidential</em> (they will be available to everyone downloading MDAnalysis) and also be aware that by adding them to MDAnalysis <em>you license these files</em> under the <a class="reference external" href="http://www.gnu.org/licenses/gpl-2.0.html">GNU Public Licence v2</a> (or a compatible licence of your choice — otherwise we cannot include the files into MDAnalysis).</p></li>
<li><p>Add the files to the <code class="docutils literal notranslate"><span class="pre">testsuite/MDAnalysisTests/data</span></code> directory and appropriate file names and descriptions to <code class="docutils literal notranslate"><span class="pre">testsuite/MDAnalysisTests/datafiles.py</span></code>.</p></li>
<li><p>Make sure your new files are picked up by the pattern-matching in <code class="docutils literal notranslate"><span class="pre">testsuite/setup.py</span></code> (in the <code class="docutils literal notranslate"><span class="pre">package_data</span></code> dictionary).</p></li>
</ol>
</div></blockquote>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="references.html" class="btn btn-neutral float-right" title="References" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="module_imports.html" class="btn btn-neutral float-left" title="Module imports in MDAnalysis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019-2020, Lily Wang, Rocco Meli, Richard J. Gowers, and Oliver Beckstein.

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  <script>
    var versions_json_url = 'https://docs.mdanalysis.org/versions.json'
</script>

<div class="rst-versions" data-toggle="rst-versions" role="note"
     aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"></span>
        
      <span class="fa fa-caret-down"></span>
    </span>

    <div class="rst-other-versions">
        <dl id="versionselector">
            <dt>Other Versions</dt>
        </dl>

    </div>
</div>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>